# _data/research.yml

# 主页面总体介绍
introduction: |
  Our research group is dedicated to advancing the frontiers of embodied AI, aiming to build intelligent agents capable of autonomous perception, decision-making, and interaction within open, dynamic, and complex physical environments. We focus on transcending the limitations of traditional static perception by exploring the 'perception–decision–action–feedback' closed loop. Our goal is to develop more general, proactive, and efficient embodied AI systems that can flexibly adapt to the complex and ever-changing real world, achieving general, active, efficient environmental perception and precise navigation.

  Our research efforts primarily revolve around the following three closely interconnected directions:

  1. **Embodied Perception**
     Exploring how agents can acquire higher-quality perceptual information through active interaction with the environment, continuously learn new knowledge, and achieve efficient, selective perception based on task requirements. Core research directions include active visual perception, continual learning for perception, and task-driven visual perception.

  2. **Visual Navigation**
     Investigating how agents understand natural language instructions, perform spatial reasoning, predict dynamic object motion, understand environmental affordances, and ultimately plan safe and efficient movement paths in complex scenes. Simultaneously, we focus on how agents can adapt to unknown environments through evolutionary learning.

  3. **Cross-Disciplinary Exploration**
     Integrating cutting-edge technologies such as world models, vision-language navigation, and vision-language-action models to promote the development of next-generation embodied AI systems that are safer, more reliable, and possess zero-shot generalization capabilities.

# 研究方向列表
research_areas:
  - id: "embodied-perception"
    name: "Embodied Perception"
    summary: ""
    image: "/images/research_img/具身感知.jpg"
    description: "Embodied perception breaks the framework of traditional static image recognition by integrating the perceptual process into the interactive loop between the agent and the environment. This allows the agent to dynamically adjust its perception strategies according to task demands, shifting from 'passively receiving information' to 'actively acquiring information.' Our objective is to endow agents with generalized perception capabilities, making them adaptable to the diverse requirements of multi-task scenarios while balancing perception efficiency and robustness."
    
    # 详细介绍页面的内容组
    content_groups:
      - title: "Active Visual Perception"
        # image: "/images/research_img/active_visual.jpg"
        caption: "Optimizing information acquisition by actively controlling sensor movement."
        description: |
          Addressing the limitations of passive perception, such as restricted viewpoints and incomplete information, we research how to optimize information acquisition by actively controlling sensor movement. This aims to obtain more complete and clearer observations of targets, enhancing the robustness of recognition and interaction.

          Our approach enables agents to dynamically adjust their perception strategies based on task requirements, moving beyond static observation to achieve more efficient and targeted information gathering.

      - title: "Continual Perceptual Learning"
        # image: "/images/research_img/continual_learning.jpg"
        caption: "Building lifelong learning capabilities for embodied agents."
        description: |
          We study how agents can effectively learn new knowledge and avoid 'catastrophic forgetting' of old knowledge when continuously encountering new categories and tasks, gradually building general perception and lifelong learning capabilities.

          Our research focuses on developing algorithms that enable continuous adaptation to changing environments while maintaining performance on previously learned tasks.

      - title: "Task-Driven Visual Perception"
        # image: "/images/research_img/task_driven.jpg"
        caption: "Achieving precise perception based on task objectives."
        description: |
          Addressing the need for efficient perception in complex embodied tasks, we investigate how to trim redundant information and achieve precise perception based on task objectives. By adaptively adjusting the computational resources and regions of interest of the perception system, we aim to balance perception speed and accuracy.

          This approach enables agents to focus on task-relevant information while ignoring irrelevant details, improving overall system efficiency.

  - id: "visual-navigation"
    name: "Visual Navigation"
    summary: ""
    image: "/images/research_img/视觉导航.jpg"
    description: "Visual navigation in complex scenes is a core capability of embodied AI. We are committed to researching how agents, relying solely on visual sensors, can understand high-level instructions (e.g., natural language) and plan and execute safe, efficient movement in open scenes filled with uncertainties (e.g., complex layouts, dynamic pedestrians, unknown traversability). The core challenge lies in building navigation systems that can integrate semantic understanding, motion prediction, and environmental affordance inference."
    
    content_groups:
      - title: "Semantic Understanding and Spatial Reasoning"
        # image: "/images/research_img/semantic_spatial.jpg"
        caption: "Parsing spatial semantics and grounding them to visual observations."
        description: |
          We explore how agents parse the spatial semantics within natural language instructions and ground them to visual observations, performing spatial reasoning by 'building a mental map.' We aim to break through the limitations of the first-person perspective and construct cross-modal, multi-scale spatial representation and reasoning capabilities.

          Our research enables agents to understand complex instructions like 'go to the red chair next to the window' and navigate accordingly.

      - title: "Motion Prediction"
        # image: "/images/research_img/motion_prediction.jpg"
        caption: "Predicting future trajectories of surrounding agents."
        description: |
          To enable safe navigation in dynamic environments, agents need the ability to predict the future trajectories of surrounding agents (e.g., pedestrians, vehicles). Our research develops algorithms that can anticipate movement patterns and adjust navigation strategies accordingly.

          This capability is crucial for applications in crowded environments where collision avoidance is essential.

      - title: "Affordance Inference"
        # image: "/images/research_img/affordance.jpg"
        caption: "Understanding environmental possibilities for action."
        description: |
          We research how agents understand the 'affordances' of the environment—the possibilities for action that the environment offers them—covering terrain traversability, object interactivity, and the functional properties of target objects.

          This understanding enables agents to make informed decisions about which paths are navigable and which objects can be interacted with.

      - title: "Evolutionary Learning for Navigation"
        # image: "/images/research_img/evolutionary.jpg"
        caption: "Autonomous evolution of navigation capabilities."
        description: |
          We explore how agents can autonomously evolve their navigation capabilities during the testing phase (i.e., when performing tasks in new environments) through online adaptation or memory-reflection mechanisms, to overcome the distribution shift between training and testing scenarios.

          This approach enables agents to continuously improve their performance in novel environments without requiring extensive retraining.

  - id: "cross-disciplinary"
    name: "Cross-Disciplinary Exploration"
    summary: ""
    image: "/images/research_img/跨领域探索.jpg"
    description: "Building upon the core technologies of embodied perception and navigation, we further expand into related research areas such as multimodal fusion and system deployment. We aim to construct a complete research chain from 'basic theory – core technology – practical application,' promoting the transition of embodied AI technology from the laboratory to real-world scenarios."
    
    content_groups:
      - title: "Integration of Multimodal Large Models and Embodied AI"
        # image: "/images/research_img/multimodal_integration.jpg"
        caption: "Deep fusion mechanisms of multimodal information."
        description: |
          We explore the deep fusion mechanisms of multimodal information (e.g., language, vision, spatial) to enhance agents' understanding and decision-making capabilities. Research directions include MLLM-based instruction parsing for vision-language navigation, cross-modal map representation learning, and natural language-driven fine-grained perception.

          Leveraging the knowledge transfer capabilities of multimodal large models, we strive to reduce system training costs and enhance zero-shot generalization capabilities.

      - title: "World Model Pre-training and Safe Planning"
        # image: "/images/research_img/world_model.jpg"
        caption: "Environmental modeling and prediction for safe navigation."
        description: |
          We focus on agents' capabilities for environmental modeling and prediction, researching pre-training methods for general world models (e.g., the PreLAR framework) to enable accurate modeling of environmental dynamics and physical rules. Combined with reinforcement learning, we implement safe and feasible trajectory planning that ensures navigation efficiency while satisfying multiple constraints such as obstacle avoidance, speed limits, and comfort.

          This research provides key technical support for safety-critical scenarios like autonomous driving and robotic navigation in complex environments.