# Research

## 研究概述

我们课题组致力于推进具身智能前沿研究，旨在构建能够在开放、动态、复杂的物理环境中进行自主感知、决策与交互的智能体。我们聚焦于突破传统静态感知的局限，研究如何在“感知-决策-动作-反馈”的闭环中，实现更通用、更主动、更高效的具身智能系统，使其能够自主适应复杂动态的物理世界，实现通用、主动、高效的环境感知与精准导航。
image: images/research_img/image_1.png

我们的工作主要围绕以下三个紧密关联的方向展开：
1. 具身感知
研究智能体如何通过与环境的主动交互，获取更优的感知信息，并持续学习新知识，同时根据任务需求进行高效、有选择的感知。核心问题包括主动视觉感知、面向感知的连续学习与任务驱动的视觉感知。

2. 视觉导航 
研究智能体在复杂场景中如何理解自然语言指令、进行空间推理、预测动态物体运动、理解环境可供性，并最终规划出安全、高效的移动路径。同时，我们探索智能体如何通过进化学习，自适应未知环境。

3. 交叉探索
探索世界模型、视觉语言导航、VLA等前沿技术在具身智能中的应用，旨在构建更安全、更可靠、具备零样本泛化能力的下一代具身智能系统。


## 子页面一：具身感知
具身感知打破传统静态图像识别的局限，将感知融入智能体与环境的交互闭环中，使其能够根据任务需求动态调整感知策略，实现从 “被动接收信息” 到 “主动获取信息” 的转变。核心目标是赋予智能体通用化的感知能力，适配多任务场景下的差异化需求，同时保证感知效率与鲁棒性。具身感知旨在突破传统“被动”视觉的框架，将视觉感知融入“感知-决策-动作”的闭环中。我们研究智能体如何像人一样，通过主动控制传感器（如移动相机）、持续积累知识、并根据任务目标选择性关注信息，从而实现动态、高效、任务导向的感知。

核心技术方向

主动视觉感知
针对被动感知中视角局限、信息不完整的问题，研究通过主动控制传感器运动优化信息获取的方法，以获得对目标更完整、更清晰的观测，从而提升识别与交互的鲁棒性。代表性工作包括 Next Best View Selection 数据集构建与算法设计，通过调整观察视角、距离与高度，显著提升目标检测的置信度与稳定性；以及 Vision in Action 框架，基于人类演示学习最优视点与操作策略，成功应用于包中取物、精确对齐等具身任务，实现感知与动作的深度协同。

连续感知学习
研究智能体在持续接触新类别、新任务时，如何有效地学习新知识，同时避免对旧知识的“灾难性遗忘”，逐步构建通用感知能力和终身学习能力。团队研究涵盖正则化（如 OWM 正交权重修改）、动态结构（如 Progressive Neural Networks 参数隔离）、经验回放等经典技术路线，并提出基于预训练大模型的 FDR（Feature Decomposition-Recomposition）方法。该方法通过分解基类视觉特征、重组泛化新类语义，有效缓解小样本增量任务中的语义偏差，在 CUB200、miniImageNet 数据集上取得优异性能。

任务驱动的视觉感知
面向复杂具身任务的高效感知需求，研究基于任务需求的冗余信息裁剪与精准感知技术。研究如何根据当前具体任务，自适应地调整感知系统的计算资源与关注区域，实现感知速度与精度的平衡。代表性工作 Task-TP（Task-Oriented Token Pruning）通过动态门控网络裁剪无关视觉区域，在物体检测与导航任务中实现 1.7 倍推理加速；ACTIVE-O3 框架则结合多模态大模型与强化学习，通过双模式奖励驱动模型自主发现任务重点区域，大幅提升局部精确感知的准确率与效率。


## 子页面二：视觉导航
复杂场景视觉导航是具身智能的核心能力之一。我们研究智能体如何仅依靠视觉传感器，在充满不确定性（如复杂布局、动态行人、未知可通行性）的开放场景中，理解高层指令（如自然语言），并规划执行安全、高效的运动。研究核心是构建能够融合语义理解、运动预测、环境可供性推断的导航系统，使智能体能够在未知或动态环境中自主规划安全、高效的路径。

核心技术方向

语义理解与空间推理
研究智能体如何解析自然语言指令中的空间语义，并与其观测到的视觉环境进行关联（Grouding），在脑中“构建地图”并进行空间推理。突破第一人称视角的局限，构建跨模态、多尺度的空间表征与推理能力。代表性工作包括 CM2（Cross-Modal Map Learning），通过语言指令与视觉信息的跨模态注意力建模，实现语义地图预测与路径规划；BEVBert 混合局部度量地图与全局拓扑地图，有效支撑长距离导航中的多阶段指令理解；TopV-nav 框架则基于俯视语义图与多模态大模型，实现物体共现关系、房间布局的全局推理，显著提升零样本物体导航性能。此外，Layout-based 因果推理方法通过 KL 散度量化布局差异，动态调控经验知识的影响，增强导航系统对陌生环境的适应性。

运动预测
为了在动态环境中安全导航，智能体必须能够预测周围其他智能体（如行人、车辆）的未来运动轨迹。HPNet（Historical Prediction Attention）框架，通过注意力机制融合历史预测信息，解决传统静态预测的时序不一致问题，在 INTERACTION、Argoverse1 数据集上取得领先性能；Plan-R1 则将运动预测作为世界模型，结合强化学习实现安全可行的轨迹规划，在 nuPlan 数据集上达到 SOTA 水平，有效降低碰撞风险。

可供性推断
研究智能体如何理解环境的“可供性”，即环境为智能体提供的行动可能性，包括地形可通行性、物体可交互性以及目标物体的功能属性。研究环境与物体对智能体的 “行动可能性”，为导航决策提供功能性指导。方向涵盖地形可供性（如 ViNT 基础模型通过时序距离评估路面可通行性，零样本适配多种机器人本体）、物体交互可供性（如效果导向型交互式导航方法，通过构建多维度可供性地图，实现动态环境中的高效路径规划）、目标可供性（如 GAMap 框架，融合几何属性与功能属性引导，提升部分可见目标的导航成功率）。

导航的进化学习
研究智能体如何在测试阶段（即在新环境中执行任务时），通过在线自适应或记忆反思机制，自主进化其导航能力，克服训练-测试场景的分布偏移问题。解决视觉语言导航中的 “分布偏移” 问题，提升智能体在新场景中的泛化能力。代表性工作 FSTTA（Fast-Slow Test-Time Adaptation）通过快慢双阶段参数更新，在测试阶段自适应新场景数据分布，平衡适应性与稳定性；SE-VLN 框架则基于多模态大模型构建层次化记忆模块与检索增强推理机制，通过闭环反思持续优化导航策略，实现系统自主进化。


## 子页面三：其他研究
围绕具身感知与导航的核心技术，拓展多模态融合、智能系统落地等相关研究方向，构建 “基础理论 - 核心技术 - 实际应用” 的完整研究链条，推动具身智能技术从实验室走向实际场景。

核心研究内容

多模态大模型与具身智能融合
探索语言、视觉、空间等多模态信息的深度融合机制，提升智能体的理解与决策能力。研究包括基于 MLLM 的视觉语言导航指令解析、跨模态地图表征学习、自然语言驱动的细粒度感知等，通过多模态大模型的知识迁移，降低具身智能系统的训练成本，提升零样本泛化能力。

世界模型预训练与安全规划
聚焦智能体对环境的建模与预测能力，研究通用世界模型的预训练方法（如 PreLAR 框架），使其能够精准建模环境动态与物理规则。结合强化学习实现安全可行的轨迹规划，在保证导航效率的同时，满足避障、限速、舒适性等约束条件，为自动驾驶等安全敏感场景提供技术支撑。



# Research

## 研究概述

本课题组致力于推动具身智能的前沿研究，旨在构建能够在开放、动态、复杂的物理环境中实现自主感知、决策与交互的智能体。我们聚焦于突破传统静态感知的局限，探索在“感知–决策–动作–反馈”的闭环中，实现更通用、主动与高效的具身智能系统，使其能够灵活适应复杂多变的现实世界，完成通用、主动、高效的环境感知与精准导航。

我们的研究工作主要围绕以下三个紧密关联的方向展开：

1. **具身感知**  
   探索智能体如何通过与环境的主动交互，获取更优质的感知信息，并持续学习新知识，同时根据任务需求实现高效、有选择的感知。核心研究方向包括主动视觉感知、面向感知的持续学习，以及任务驱动的视觉感知。

2. **视觉导航**  
   研究智能体在复杂场景中如何理解自然语言指令、进行空间推理、预测动态物体运动、理解环境可供性，并最终规划出安全高效的移动路径。同时，我们关注智能体如何通过进化学习适应未知环境。

3. **交叉探索**  
   融合世界模型、视觉语言导航、视觉-语言-动作模型等前沿技术，推动构建更安全、可靠、具备零样本泛化能力的下一代具身智能系统。

---

## 子页面一：具身感知

具身感知突破了传统静态图像识别的框架，将感知过程融入智能体与环境的交互闭环中，使其能够根据任务需求动态调整感知策略，实现从“被动接收信息”向“主动获取信息”的转变。我们的目标是赋予智能体通用化的感知能力，使其适配多任务场景下的差异化需求，同时兼顾感知效率与鲁棒性。

### 核心技术方向

**主动视觉感知**  
针对被动感知中视角局限和信息不完整的问题，我们研究如何通过主动控制传感器运动来优化信息获取，从而获得对目标更完整、更清晰的观测，提升识别与交互的鲁棒性。代表性工作包括“Next Best View Selection”数据集构建与算法设计，通过调整观察视角、距离与高度显著提升目标检测的置信度与稳定性；以及“Vision in Action”框架，基于人类演示学习最优视点与操作策略，已成功应用于包中取物、精确对齐等具身任务，实现感知与动作的深度协同。

**连续感知学习**  
我们研究智能体在持续接触新类别、新任务时，如何有效学习新知识并避免对旧知识的“灾难性遗忘”，逐步构建通用感知与终身学习能力。团队研究涵盖正则化、动态网络结构、经验回放等经典方法，并提出基于预训练大模型的 FDR（特征分解-重组）方法。该方法通过分解基类视觉特征并重组泛化新类语义，有效缓解小样本增量任务中的语义偏差，在 CUB200、miniImageNet 等数据集上表现优异。

**任务驱动的视觉感知**  
面向复杂具身任务的高效感知需求，我们研究如何根据任务目标裁剪冗余信息、实现精准感知。通过自适应调整感知系统的计算资源与关注区域，达成感知速度与精度的平衡。代表性工作 Task-TP（任务导向的令牌剪枝）通过动态门控网络剔除无关视觉区域，在物体检测与导航任务中实现 1.7 倍推理加速；ACTIVE-O3 框架则融合多模态大模型与强化学习，通过双模式奖励机制引导模型自主聚焦任务关键区域，显著提升局部感知的准确率与效率。

---

## 子页面二：视觉导航

复杂场景下的视觉导航是具身智能的核心能力。我们致力于研究智能体如何仅依赖视觉传感器，在充满不确定性（如复杂布局、动态行人、未知可通行性）的开放场景中，理解高层指令（如自然语言），并规划执行安全、高效的移动。核心在于构建能够融合语义理解、运动预测与环境可供性推断的导航系统，使智能体在未知或动态环境中实现自主路径规划。

### 核心技术方向

**语义理解与空间推理**  
我们探索智能体如何解析自然语言指令中的空间语义，并将其与视觉观测进行关联（Grouding），在“脑中构图”并执行空间推理。突破第一人称视角局限，构建跨模态、多尺度的空间表征与推理能力。代表性工作包括 CM2（跨模态地图学习），通过语言与视觉的跨模态注意力建模，实现语义地图预测与路径规划；BEVBert 融合局部度量地图与全局拓扑地图，支持长距离导航中的多阶段指令理解；TopV-nav 框架则基于俯视语义图与多模态大模型，实现物体共现关系与房间布局的全局推理，显著提升零样本物体导航性能。此外，基于布局的因果推理方法通过 KL 散度量化布局差异，动态调控经验知识的影响，增强系统对陌生环境的适应能力。

**运动预测**  
为在动态环境中实现安全导航，智能体需能够预测周围智能体（如行人、车辆）的未来运动轨迹。HPNet（历史预测注意力）框架通过注意力机制融合历史预测信息，解决传统静态预测的时序不一致问题，在 INTERACTION、Argoverse1 等数据集上取得领先性能；Plan-R1 将运动预测作为世界模型，结合强化学习实现安全可行的轨迹规划，在 nuPlan 数据集上达到 SOTA 水平，显著降低碰撞风险。

**可供性推断**  
我们研究智能体如何理解环境的“可供性”，即环境为其提供的行动可能性，涵盖地形可通行性、物体可交互性以及目标物体的功能属性。相关研究包括地形可供性（如 ViNT 基础模型通过时序距离评估路面可通行性，实现零样本适配多种机器人平台）、物体交互可供性（如效果导向型交互导航方法，通过构建多维度可供性地图实现动态环境中的高效路径规划），以及目标可供性（如 GAMap 框架融合几何与功能属性引导，提升部分可见目标的导航成功率）。

**导航的进化学习**  
我们探索智能体在测试阶段（即在新环境中执行任务时）如何通过在线自适应或记忆反思机制，自主进化其导航能力，以克服训练–测试场景间的分布偏移问题。代表性工作 FSTTA（快慢测试时适应）通过双阶段参数更新，在测试阶段自适应新场景数据分布，平衡适应性与稳定性；SE-VLN 框架基于多模态大模型构建层次化记忆模块与检索增强推理机制，通过闭环反思持续优化导航策略，实现系统的自主进化。

---

## 子页面三：其他研究

围绕具身感知与导航核心技术，我们进一步拓展多模态融合、系统落地等相关研究方向，构建从“基础理论–核心技术–实际应用”的完整研究链条，推动具身智能技术走出实验室，服务真实场景。

### 核心研究内容

**多模态大模型与具身智能融合**  
我们探索语言、视觉、空间等多模态信息的深度融合机制，以增强智能体的理解与决策能力。研究方向包括基于 MLLM 的视觉语言导航指令解析、跨模态地图表征学习、自然语言驱动的细粒度感知等。借助多模态大模型的知识迁移能力，我们致力于降低系统训练成本，提升零样本泛化能力。

**世界模型预训练与安全规划**  
我们聚焦于智能体对环境的建模与预测能力，研究通用世界模型的预训练方法（如 PreLAR 框架），使其能够准确建模环境动态与物理规则。结合强化学习，我们实现安全可行的轨迹规划，在保障导航效率的同时，满足避障、限速、舒适性等多重约束，为自动驾驶等安全敏感场景提供关键技术支撑。








# Research

## 研究概述

本课题组致力于推动具身智能的前沿研究，旨在构建能够在开放、动态、复杂的物理环境中实现自主感知、决策与交互的智能体。我们聚焦于突破传统静态感知的局限，探索在“感知–决策–动作–反馈”的闭环中，实现更通用、主动与高效的具身智能系统，使其能够灵活适应复杂多变的现实世界，完成通用、主动、高效的环境感知与精准导航。

我们的研究工作主要围绕以下三个紧密关联的方向展开：

1. **具身感知**  
   探索智能体如何通过与环境的主动交互，获取更优质的感知信息，并持续学习新知识，同时根据任务需求实现高效、有选择的感知。核心研究方向包括主动视觉感知、面向感知的持续学习，以及任务驱动的视觉感知。

2. **视觉导航**  
   研究智能体在复杂场景中如何理解自然语言指令、进行空间推理、预测动态物体运动、理解环境可供性，并最终规划出安全高效的移动路径。同时，我们关注智能体如何通过进化学习适应未知环境。

3. **交叉探索**  
   融合世界模型、视觉语言导航、视觉-语言-动作模型等前沿技术，推动构建更安全、可靠、具备零样本泛化能力的下一代具身智能系统。

---

## 子页面一：具身感知

具身感知突破了传统静态图像识别的框架，将感知过程融入智能体与环境的交互闭环中，使其能够根据任务需求动态调整感知策略，实现从“被动接收信息”向“主动获取信息”的转变。我们的目标是赋予智能体通用化的感知能力，使其适配多任务场景下的差异化需求，同时兼顾感知效率与鲁棒性。

### 核心技术方向

**主动视觉感知**  
针对被动感知中视角局限和信息不完整的问题，我们研究如何通过主动控制传感器运动来优化信息获取，从而获得对目标更完整、更清晰的观测，提升识别与交互的鲁棒性。

**连续感知学习**  
我们研究智能体在持续接触新类别、新任务时，如何有效学习新知识并避免对旧知识的“灾难性遗忘”，逐步构建通用感知与终身学习能力。

**任务驱动的视觉感知**  
面向复杂具身任务的高效感知需求，我们研究如何根据任务目标裁剪冗余信息、实现精准感知。通过自适应调整感知系统的计算资源与关注区域，达成感知速度与精度的平衡。

---

## 子页面二：视觉导航

复杂场景下的视觉导航是具身智能的核心能力。我们致力于研究智能体如何仅依赖视觉传感器，在充满不确定性（如复杂布局、动态行人、未知可通行性）的开放场景中，理解高层指令（如自然语言），并规划执行安全、高效的移动。核心在于构建能够融合语义理解、运动预测与环境可供性推断的导航系统，使智能体在未知或动态环境中实现自主路径规划。

### 核心技术方向

**语义理解与空间推理**  
我们探索智能体如何解析自然语言指令中的空间语义，并将其与视觉观测进行关联（Grouding），在“脑中构图”并执行空间推理。突破第一人称视角局限，构建跨模态、多尺度的空间表征与推理能力。

**运动预测**  
为在动态环境中实现安全导航，智能体需能够预测周围智能体（如行人、车辆）的未来运动轨迹。

**可供性推断**  
我们研究智能体如何理解环境的“可供性”，即环境为其提供的行动可能性，涵盖地形可通行性、物体可交互性以及目标物体的功能属性。

**导航的进化学习**  
我们探索智能体在测试阶段（即在新环境中执行任务时）如何通过在线自适应或记忆反思机制，自主进化其导航能力，以克服训练–测试场景间的分布偏移问题。

---

## 子页面三：其他研究

围绕具身感知与导航核心技术，我们进一步拓展多模态融合、系统落地等相关研究方向，构建从“基础理论–核心技术–实际应用”的完整研究链条，推动具身智能技术走出实验室，服务真实场景。

### 核心研究内容

**多模态大模型与具身智能融合**  
我们探索语言、视觉、空间等多模态信息的深度融合机制，以增强智能体的理解与决策能力。研究方向包括基于 MLLM 的视觉语言导航指令解析、跨模态地图表征学习、自然语言驱动的细粒度感知等。借助多模态大模型的知识迁移能力，我们致力于降低系统训练成本，提升零样本泛化能力。

**世界模型预训练与安全规划**  
我们聚焦于智能体对环境的建模与预测能力，研究通用世界模型的预训练方法（如 PreLAR 框架），使其能够准确建模环境动态与物理规则。结合强化学习，我们实现安全可行的轨迹规划，在保障导航效率的同时，满足避障、限速、舒适性等多重约束，为自动驾驶等安全敏感场景提供关键技术支撑。